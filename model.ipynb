{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/KaNSsOb9fWtJX2UZ2MVV"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kh4su5Hknvnj"
      },
      "outputs": [],
      "source": [
        "!pip install openai --quiet\n",
        "!pip install matplotlib --quiet\n",
        "!pip install scipy --quiet\n",
        "!pip install scikit-learn --quiet\n",
        "!pip install singlestoredb --quiet\n",
        "!pip install langchain --quiet\n",
        "!pip install pymongo --quiet\n",
        "!pip install backoff --quiet\n",
        "!pip install tenacity --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pymongo\n",
        "from pymongo import MongoClient\n",
        "import pandas as pd\n",
        "# create connections to mongo & s2\n",
        "myclientmongodb = pymongo.MongoClient(\"mongodb+srv://daviddaeshinlee:Password123!@genai-demo-dlee.56ode.mongodb.net/?retryWrites=true&w=majority\")\n",
        "s2clientmongodb = pymongo.MongoClient(\"mongodb://admin:Admin!1010@svc-e031280b-9434-4aea-8e36-ecc206b87e7a-mongo.aws-mumbai-1.svc.singlestore.com:27017/?authMechanism=PLAIN&tls=true&loadBalanced=true\")\n",
        "\n",
        "#\n",
        "mongodb_db = myclientmongodb[\"test\"]\n",
        "mongocollection = mongodb_db[\"stock_data_updated\"]\n",
        "\n",
        "s2_db = s2clientmongodb[\"ai_demo\"]\n",
        "s2table = s2_db[\"mongo_json\"]\n",
        "\n",
        "# write in singlestore\n",
        "df = pd.DataFrame(list(mongocollection.find().skip(1600000)))\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "data_dict = df.to_dict(\"records\")\n",
        "s2table.insert_many(data_dict)"
      ],
      "metadata": {
        "id": "pKPAT9GWn8Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  sample relational data\n",
        "%%sql\n",
        "select * from mongo_json limit 2"
      ],
      "metadata": {
        "id": "CPvIjMzso2i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if all the data is successfully migrated\n",
        "import pymongo\n",
        "from pymongo.mongo_client import MongoClient\n",
        "import requests\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "# create connections to mongo & s2\n",
        "myclientmongodb = pymongo.MongoClient(\"mongodb+srv://daviddaeshinlee:Password123!@genai-demo-dlee.56ode.mongodb.net/?retryWrites=true&w=majority\")\n",
        "s2clientmongodb = pymongo.MongoClient(\"mongodb://admin:Admin!1010@svc-e031280b-9434-4aea-8e36-ecc206b87e7a-mongo.aws-mumbai-1.svc.singlestore.com:27017/?authMechanism=PLAIN&tls=true&loadBalanced=true\")\n",
        "\n",
        "# Connect to collection\n",
        "mongodb_db = myclientmongodb[\"test\"]\n",
        "mongocollection = mongodb_db[\"stock_data_updated\"]\n",
        "\n",
        "\n",
        "s2_db = s2clientmongodb[\"ai_demo\"]\n",
        "s2table = s2_db[\"mongo_json\"]\n",
        "\n",
        "# count_documents to check if all the data is successfully transfered\n",
        "records = mongocollection.count_documents({})\n",
        "print(f'Number of records in Mongo: {records}')\n",
        "\n",
        "s2records = s2table.count_documents({})\n",
        "print(f'Number of records in SingleStore: {s2records}')"
      ],
      "metadata": {
        "id": "wCnzlULxpCC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bringing news feeds to database"
      ],
      "metadata": {
        "id": "L-FzBBntpoXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "import openai\n",
        "import os\n",
        "import pandas as pd\n",
        "from langchain import PromptTemplate , OpenAI , LLMChain\n",
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_random_exponential,\n",
        ")  # for exponential backoff\n",
        "import backoff\n",
        "\n",
        "model = 'text-embedding-ada-002'\n",
        "\n",
        "# %%sql\n",
        "# create table embeddings (\n",
        "#     `id` int auto_increment primary key,\n",
        "#     `category` varchar(255),\n",
        "#     `question` longtext,\n",
        "#     `question_embedding` longblob,\n",
        "#     `answer` longtext,\n",
        "#     `answer_embedding` longblob,\n",
        "#     `created_at` datetime\n",
        "#     );\n",
        "# functions parse articles and chunk the text\n",
        "def parse_title_summary_results(results):\n",
        "    out = []\n",
        "    for e in results:\n",
        "        e = e.replace('\\n', '')\n",
        "        out.append(e)\n",
        "\n",
        "    return out\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"Insert OpenAI API key here\"\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "\n",
        "def summarize_stage_1(chunks_text):\n",
        "\n",
        "    # Prompt to get title and summary for each chunk\n",
        "    map_prompt_template = \"\"\"Pretend you're a financial analyst. Given the following text, give me a prompt that would output this answer: {text}\"\"\"\n",
        "    map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
        "\n",
        "    # Define the LLMs\n",
        "    map_llm = OpenAI(temperature=0, model_name = 'text-davinci-002')\n",
        "    map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
        "    map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
        "    # Run the input through the LLM chain (works in parallel)\n",
        "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
        "    return map_llm_chain_results\n",
        "\n",
        "\n",
        "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
        "\n",
        "@retry(stop=stop_after_attempt(2), wait=wait_random_exponential(min=60, max=80))\n",
        "def get_embedding(text, model=model):\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    return openai.Embedding.create(input=[text], model=model)['data'][0]['embedding']\n",
        "\n",
        "## Script Start ##\n",
        "\n",
        "# Pull data from API, then chunk it and insert into a dataframe\n",
        "companies = ['Amazon']\n",
        "\n",
        "article_id_values = []\n",
        "url_values = []\n",
        "pubdate_values = []\n",
        "description_values = []\n",
        "summary_values = []\n",
        "content_values = []\n",
        "companies_name_values = []\n",
        "\n",
        "for company in companies:\n",
        "\n",
        "    print(company)\n",
        "    url = \"http://api.goperigon.com/v1/all?apiKey=<API Key here>&country=us&sourceGroup=top25finance&excludeSource=benzinga.com&showReprints=false&paywall=false&language=en&companyName=\" + company + \"&medium=Article?&excludeLabel=Non-news&excludeLabel=Opinion&excludeLabel=PaidNews&excludeLabel=Roundup&language=en&medium=Article\"\n",
        "    resp = requests.get(url)\n",
        "\n",
        "    article_json = resp.json()[\"articles\"]\n",
        "\n",
        "    for article in article_json:\n",
        "        # Chuck article on every 1,000 characters\n",
        "        full_content = article['content']\n",
        "        n_characters = len(full_content)\n",
        "        print(\"No of characters is \", n_characters)\n",
        "        chunk_size = 1000\n",
        "        n_chunks = n_characters//chunk_size + 1\n",
        "\n",
        "        # print(article_json)\n",
        "\n",
        "        i = 0\n",
        "        while i < chunk_size * n_chunks:\n",
        "            new_chunk = full_content[i:i+chunk_size]\n",
        "            content_values += [new_chunk]\n",
        "            i += chunk_size\n",
        "\n",
        "        # Insert values into lists\n",
        "        article_id_values += n_chunks * [article['articleId']]\n",
        "        url_values += n_chunks * [article['url']]\n",
        "        pubdate_values += n_chunks * [article['pubDate']]\n",
        "        description_values += n_chunks * [article['description']]\n",
        "        summary_values += n_chunks * [article['summary']]\n",
        "        companies_name_values += n_chunks * [company]\n",
        "\n",
        "# Convert group of lists into a dataframe\n",
        "data_dict = {'article_id':article_id_values, 'url':url_values, 'pubdate':pubdate_values,\n",
        "             'description':description_values, 'summary':summary_values, 'content':content_values,\n",
        "             'company':companies_name_values}\n",
        "\n",
        "df = pd.DataFrame(data_dict)\n",
        "\n",
        "# Get prompts for each chuck, clean results from Open AI then add new \"prompt\" column\n",
        "content_chunks = df[['content']].values.tolist()\n",
        "stage_1_outputs = summarize_stage_1(content_chunks)\n",
        "\n",
        "prompts = parse_title_summary_results([e['text'] for e in stage_1_outputs])\n",
        "df['question'] = prompts\n",
        "df.to_csv('stage1.csv')\n",
        "# get the vector embedding for the prompt & content chuck then add to dataframe\n",
        "prompt_embedding = []\n",
        "chunk_embedding = []\n",
        "\n",
        "print(\"prompts\",len(prompts))\n",
        "count=2\n",
        "for i in range(len(prompts)):\n",
        "    print(\"prompt count\", i)\n",
        "    if count == 0:\n",
        "        time.sleep(80)\n",
        "        count = 2\n",
        "\n",
        "    count -= 1\n",
        "    # prompt = prompts[i]\n",
        "    # prompt_embedding.append(get_embedding(prompt, model=model))\n",
        "    content_chunk = content_chunks[i][0]\n",
        "    chunk_embedding.append(get_embedding(content_chunk, model=model))\n",
        "\n",
        "    time.sleep(10)\n",
        "\n",
        "\n",
        "df['question_embedding'] = prompt_embedding\n",
        "df['answer_embedding'] = chunk_embedding\n",
        "# Store the relational data from api along with generated questions and embeddings to a csv file\n",
        "df.to_csv('stage3.csv')"
      ],
      "metadata": {
        "id": "DYuZU6lDp_Xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataframe1 = pd.read_csv('news_embeddings.csv')\n",
        "print(dataframe1)"
      ],
      "metadata": {
        "id": "YV5cszMxc6Lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac0a6132-1b04-4bd7-bee7-bd62c504fdf8"
      },
      "outputs": [],
      "source": [
        "# Clean dataframe before inserting into embeddings table\n",
        "new_df = dataframe1[['article_id', 'question', 'question_embedding', 'content', 'answer_embedding', 'pubdate', 'company']]\n",
        "new_df['pubdate'] = pd.to_datetime(new_df['pubdate'], format='%Y-%m-%dT%H:%M:%S')\n",
        "new_df['pubdate'] = new_df['pubdate'].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "new_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymysql\n",
        "!pip install sqlalchemy"
      ],
      "metadata": {
        "id": "x3oJni15d8l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Push final dataframe to embeddings table\n",
        "from sqlalchemy import create_engine, Column, Integer, String, BLOB, JSON , text\n",
        "from sqlalchemy.ext.declarative import declarative_base\n",
        "from sqlalchemy.orm import sessionmaker\n",
        "from datetime import datetime\n",
        "import pymysql\n",
        "user = 'admin'\n",
        "password = 'Password123!'\n",
        "host ='svc-e031280b-9434-4aea-8e36-ecc206b87e7a-dml.aws-mumbai-1.svc.singlestore.com'\n",
        "port = 3306\n",
        "database = 'ai_demo'\n",
        "table_name = 'embeddings'\n",
        "model = 'text-embedding-ada-002'\n",
        "engine = create_engine(f\"mysql+pymysql://{user}:{password}@{host}:{port}/{database}\")\n",
        "\n",
        "# Insert to SingleStore\n",
        "mystmt = \"INSERT INTO {} (category, question, question_embedding, answer, answer_embedding, created_at) VALUES ('{}', '{}', json_array_pack('{}'), '{}', json_array_pack('{}'), '{}');\"\n",
        "\n",
        "for i in range(len(new_df)):\n",
        "\n",
        "    stmt = mystmt.format(table_name, new_df['company'][i],new_df['question'][i].replace(\"'\",\"\"), new_df['question_embedding'][i], new_df['content'][i].replace(\"'\",\"\"), new_df['answer_embedding'][i], new_df['pubdate'][i])\n",
        "    # print(stmt)\n",
        "\n",
        "    with engine.connect() as conn:\n",
        "         conn.execute(text(stmt))\n",
        "         conn.commit()"
      ],
      "metadata": {
        "id": "eWhbGBfOdcO2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}